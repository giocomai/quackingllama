[{"path":"https://giocomai.github.io/quackingllama/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 quackingllama authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://giocomai.github.io/quackingllama/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Giorgio Comai. Author, maintainer, copyright holder.","code":""},{"path":"https://giocomai.github.io/quackingllama/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Comai G (2025). quackingllama: Process text Ollama, retrieve structured results, cache locally DuckDB. R package version 0.0.0.9014, https://giocomai.github.io/quackingllama/.","code":"@Manual{,   title = {quackingllama: Process text with Ollama, retrieve structured results, cache them locally in DuckDB},   author = {Giorgio Comai},   year = {2025},   note = {R package version 0.0.0.9014},   url = {https://giocomai.github.io/quackingllama/}, }"},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"quackingllama-","dir":"","previous_headings":"","what":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"goal quackingllama facilitate efficient interactions LLMs; current target use-case text classification (e.g. categorise tag contents, extract information text). Key features include: facilitate consistently formatted responses (Ollama’s structured ouputs) facilitate local caching (storing results local DuckDB database) facilitate initiating text classification tasks (examples convenience functions) facilitate keeping record details response received","code":""},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"can install development version quackingllama GitHub :","code":"# install.packages(\"pak\") pak::pak(\"giocomai/quackingllama\")"},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"default-options-and-outputs","dir":"","previous_headings":"","what":"Default options and outputs","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"order facilitate consistent results, default quackingllama sets temperature model 0: means always return response given prompt. caching enabled, responses can consistently retrieved local cache without querying LLMs. functions consistently return results data frame (tibble). Key functionalities demonstrated series examples. package developed , less intuitive tasks (e.g. defining schema) facilitated dedicated convenience functions.","code":""},{"path":[]},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"text-generation","dir":"","previous_headings":"Basic examples","what":"Text generation","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"Let’s generate short piece text. Results returned data frame, response first column relevant metadata query stored along . Meet Aurora “Rory” Thompson, charismatic progressive leader coastal nation Azura. former environmental activist turned politician, Rory known unwavering commitment sustainability social justice. warm smile infectious laugh, won hearts constituents inclusive policies bold vision greener future. President Azura, Rory made mission protect planet promoting economic growth equality citizens. leadership style collaborative, empathetic, unapologetically forward-thinking. interested variations text, can easily create : , customary default behaviour LLMs, free form texts. Depending task hand, may want text structured format. , must provide LLM schema want return data. Schema can simple, e.g., want response feature “name” “description” field, character strings, ’d use following schema: slightly complex, example making clear expect field numeric, another one pick one set options: returned formatted expected: response structured format allows easily storing results data frame processing . obvious advantages many data processing tasks, , seen, can effectively used enhance consistency text classification tasks. first, let’s discuss caching options determine output.","code":"library(\"quackingllama\") pol_df <- ql_prompt(prompt = \"Describe an imaginary political leader in less than 100 words.\") |>   ql_generate()  str(pol_df) #> tibble [1 × 19] (S3: tbl_df/tbl/data.frame) #>  $ response            : chr \"Meet Aurora \\\"Rory\\\" Thompson, the charismatic and progressive leader of the coastal nation of Azura. A former \"| __truncated__ #>  $ prompt              : chr \"Describe an imaginary political leader in less than 100 words.\" #>  $ created_at          : chr \"2025-04-03T13:18:16.042489584Z\" #>  $ done                : logi TRUE #>  $ done_reason         : chr \"stop\" #>  $ total_duration      : num 2.84e+09 #>  $ load_duration       : num 20312921 #>  $ prompt_eval_count   : num 43 #>  $ prompt_eval_duration: num 57514831 #>  $ eval_count          : num 118 #>  $ eval_duration       : num 2.77e+09 #>  $ timeout             : num 300 #>  $ keep_alive          : chr \"5m\" #>  $ model               : chr \"llama3.2\" #>  $ system              : chr \"You are a helpful assistant.\" #>  $ format              : chr \"\" #>  $ seed                : num 0 #>  $ temperature         : num 0 #>  $ hash                : chr \"ff3978b23a04a61a6037ab8e66521252\" cat(\">\", pol_df$response) # TODO accept multiple prompts by default  pol3_df <- purrr::map(   .x = c(\"progressive\", \"conservative\", \"centrist\"),   .f = \\(x) {     ql_prompt(prompt = glue::glue(\"Describe an imaginary {x} politician in less than 100 words.\")) |>       ql_generate()   } ) |>   purrr::list_rbind()  pol3_df$response #> [1] \"Meet Maya Ramos, a charismatic and visionary leader who embodies the values of social justice and environmental sustainability. As a former community organizer and small business owner, Maya understands the needs of everyday people and is committed to creating economic opportunities that lift up marginalized communities. Her progressive platform prioritizes affordable healthcare, free public education, and a Green New Deal that invests in renewable energy and sustainable infrastructure. With her warm smile and infectious passion, Maya inspires a new generation of activists and voters to join the fight for a more just and equitable society.\" #> [2] \"Meet Reginald P. Bottomsworth, a stalwart conservative politician from rural America. A third-generation farmer and small business owner, Reggie is known for his down-to-earth values and no-nonsense approach to governance. He supports traditional industries like agriculture and manufacturing, and advocates for limited government intervention in personal and economic matters. With a folksy demeanor and a strong work ethic, Reggie has built a loyal following among conservative voters who appreciate his commitment to preserving American traditions and individual freedoms. His slogan? \\\"Common sense, not Washington wisdom.\\\"\"                    #> [3] \"Meet Alexandra \\\"Alex\\\" Thompson, a pragmatic and moderate politician. A former business owner turned public servant, Alex brings a unique blend of fiscal responsibility and social compassion to the table. She advocates for balanced budgets, tax reform, and investments in education and infrastructure. However, she also prioritizes affordable healthcare, environmental protection, and social justice. With a calm and collected demeanor, Alex is able to bridge partisan divides and find common ground with her constituents. Her centrist approach has earned her a reputation as a trusted mediator and problem-solver in the halls of power.\" # TODO convenience function to facilitate creation of common schemas  schema <- list(   type = \"object\",   properties = list(     `name` = list(type = \"string\"),     `description` = list(type = \"string\")   ),   required = c(\"name\", \"description\") ) pol_schema_df <- ql_prompt(   prompt = \"Describe an imaginary political leader.\",   format = schema ) |>   ql_generate()  pol_schema_df$response |>   yyjsonr::read_json_str() #> $name #> [1] \"Aurora Wynter\" #>  #> $description #> [1] \"Aurora Wynter is a charismatic and visionary leader who has captivated the hearts of her people with her unwavering commitment to justice, equality, and environmental sustainability. Born in the coastal city of Newhaven, Aurora grew up surrounded by the beauty and fragility of the ocean, which instilled in her a deep love for the natural world and a fierce determination to protect it from harm. As a young woman, she became an outspoken advocate for climate action, using her powerful voice to raise awareness about the urgent need for sustainable practices and renewable energy sources. Her message resonated with people of all ages and backgrounds, earning her a reputation as a passionate and authentic leader who is not afraid to challenge the status quo. Aurora's leadership style is characterized by her collaborative approach, which brings together diverse stakeholders to find innovative solutions to complex problems. She is known for her ability to listen deeply and empathetically, often finding common ground with even the most unlikely of opponents. Despite facing numerous challenges and setbacks throughout her career, Aurora remains steadfast in her commitment to creating a better world for all, inspiring countless individuals around the globe to join her on this journey towards a brighter future.\" schema <- list(   type = \"object\",   properties = list(     `name` = list(type = \"string\"),     `age` = list(type = \"number\"),     `gender` = list(       type = \"string\",       enum = c(\"female\", \"male\", \"non-binary\")     ),     `motto` = list(type = \"string\"),     `description` = list(type = \"string\")   ),   required = c(     \"name\",     \"age\",     \"gender\",     \"motto\",     \"description\"   ) ) pol_schema_df <- ql_prompt(   prompt = \"Describe an imaginary political leader.\",   format = schema ) |>   ql_generate()  pol_schema_df$response |>   yyjsonr::read_json_str() #> $name #> [1] \"Aurora Wynter\" #>  #> $age #> [1] 52 #>  #> $gender #> [1] \"female\" #>  #> $motto #> [1] \"Unity in Diversity, Progress through Inclusion\" #>  #> $description #> [1] \"Aurora Wynter is a charismatic and visionary leader who has captivated the hearts of her people. Born into a family of modest means, she rose to prominence as a grassroots activist, fighting for social justice and equality. Her unwavering commitment to these values has earned her the respect and admiration of her constituents. With a warm smile and an infectious laugh, Aurora exudes confidence and compassion, inspiring those around her to work towards a brighter future.\" pol3_schema_df <- purrr::map(   .x = c(\"progressive\", \"conservative\", \"centrist\"),   .f = \\(x) {     ql_prompt(       prompt = glue::glue(\"Describe an imaginary {x} politician.\"),       format = schema     ) |>       ql_generate()   } ) |>   purrr::list_rbind()  pol3_schema_responses_df <- purrr::map(   .x = pol3_schema_df$response,   .f = \\(x) {     yyjsonr::read_json_str(x) |>       tibble::as_tibble()   } ) |>   purrr::list_rbind()  pol3_schema_responses_df #> # A tibble: 3 × 5 #>   name                       age gender motto                        description #>   <chr>                    <int> <chr>  <chr>                        <chr>       #> 1 Maya Ramos                  42 female Empowering a Just and Equit… Maya Ramos… #> 2 Reginald P. Bottomsworth    55 male   Tradition, Progress, and Pr… A seasoned… #> 3 Alexander Thompson          52 male   Pragmatic Progress           Alexander …"},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"caching-and-options","dir":"","previous_headings":"Basic examples","what":"Caching and options","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"far, local caching enabled: means even exacts response expected, still requested LLM, can exceedingly time-consuming especially repetitive tasks, data processing pipelines may recurrently encounter data. Caching obvious answer process, expect exactly response LLM, considering LLMs necessarily return response even given prompt? Two parameters particularly relevant understanding , temperature seed. “temperature”? Ollama’s documentation concisely clarifies effect parameter suggesting “Increasing temperature make model answer creatively.” LLMs often default temperature set 0.7 0.8. brief, temperature set maximum value 1, LLMs provide varied responses. temperature set 0, LLMs consistent: always provide response prompt. mean practices? example, set temperature 0 ask LLM generate haiku, always get haiku, matter many times run command. set temperature 1, get every time different haiku (ok, different, really, still different). , replicability results possible even temperature set value higher 0. just need set seed, ’ll consistently get result. Two additional components determine response exactly different instances: system format. system parameter passed along prompt LLM, default set generic “helpful assistant.”. reasonable generic option, may good reasons specific depending task hand. example, set system message “19th century romantic poet.”, style response change (somewhat) accordingly. discussed , format relevant instances structured output requested LLM providing schema. example, provided different schema, output also different. brief, expect receive exactly response LLM, hence, making possible retrieve cache already parsed? following conditions must apply: model system parameter format, .e., schema (given). prompt either seed value temperature seed temperature set zero conditions met, caching enabled, response retrieved local cache, rather LLM. ’s easy enable caching current session ql_enable_db(). default, database stored current working directory, can changed ql_set_db_options(). Now even prompts take LLM many seconds process can returned efficiently cache:","code":"ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 0) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Tacos on my face\\nSalsa drips from happy lips\\nMidlife crisis born\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 0) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Tacos on my face\\nSalsa drips from happy lips\\nMidlife crisis born\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 0) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Tacos on my face\\nSalsa drips from happy lips\\nMidlife crisis born\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 1) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Tacos for my soul\\nCrunchy shell, cheesy delight\\nMidlife crisis food\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 1) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Tacos on my head\\nSalsa drips from every pore\\nMidlife crisis dance\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 1) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Taco Tuesday dreams\\nCrunchy, cheesy, saucy bliss\\nDance on the couch night\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 1, seed = 2025) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Pizza in my lap\\nMelted cheese and happy sigh\\nLife's simple delight\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 1, seed = 2025) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Pizza in my lap\\nMelted cheese and happy sigh\\nLife's simple delight\" ql_prompt(prompt = \"A reasonably funny haiku\", temperature = 1, seed = 2025) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Pizza in my lap\\nMelted cheese and happy sigh\\nLife's simple delight\" ql_prompt(   prompt = \"A reasonably funny haiku\",   temperature = 0,   system = \"You are a 19th century romantic writer.\" ) |>   ql_generate() |>   dplyr::pull(response) #> [1] \"Fop's ridiculous hat\\nTops his lumpy, love-struck face\\nSighs of wretched bliss\" schema <- list(   type = \"object\",   properties = list(     `haiku` = list(type = \"string\"),     `why_funny` = list(type = \"string\")   ),   required = c(     \"haiku\",     \"why_funny\"   ) )  haiku_str_df <- ql_prompt(   prompt = \"Write a funny haiku, and explain why it is supposed to be funny.\",   format = schema ) |> ql_generate()  haiku_str_df |>   dplyr::pull(response) |>   yyjsonr::read_json_str() #> $haiku #> [1] \"I fart in space\" #>  #> $why_funny #> [1] \"This haiku is meant to be humorous because it takes a common bodily function (farting) and combines it with an unexpected setting (space). The juxtaposition of the mundane and the extraordinary creates a comedic effect. Additionally, the simplicity and brevity of the haiku make the punchline more impactful.\" ql_enable_db() ql_set_db_options(db_folder = fs::path_home_r(\"R\"))"},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"text-classification","dir":"","previous_headings":"Basic examples","what":"Text classification","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"First, let’s create texts try classify: ask different model categorise results (example, text generation llama3.2, text categorisation mistral). Trimming explanations following table clarity. stereotyped case, LLM categorises statements expected provide broadly meaningful explanation choice (try shorter sentences, e.g., just political motto, correct response rate decreases substantially). Fundamentally: responses returned predictable user-defined format, consistently responding user-defined categories re-running categorisation task efficient categorisation large set texts can interrupted , already processed contents categorised . Querying different models can substantial impact quality results.","code":"schema <- list(   type = \"object\",   properties = list(     `party name` = list(type = \"string\"),     `political leaning` = list(       type = \"string\",       enum = c(\"progressive\", \"conservative\")     ),     `political statement` = list(type = \"string\")   ),   required = c(     \"party name\",     \"political leaning\",     \"political statement\"   ) )  parties_df <- purrr::map2(   .x = rep(c(\"progressive\", \"conservative\"), 5),   .y = 1:10,   .f = \\(x, y) {     ql_prompt(       prompt = glue::glue(\"Describe an imaginary {x} political party, inventing their party name and a characteristic political statement.\"),       format = schema,       temperature = 1,       seed = y     ) |>       ql_generate()   } ) |>   purrr::list_rbind()  parties_responses_df <- purrr::map(   .x = parties_df$response,   .f = \\(x) {     yyjsonr::read_json_str(x) |>       tibble::as_tibble()   } ) |>   purrr::list_rbind()  parties_responses_df #> # A tibble: 10 × 3 #>    `party name`                    `political leaning` `political statement`     #>    <chr>                           <chr>               <chr>                     #>  1 The Luminari Party              progressive         \"Embracing a global citi… #>  2 The Liberty Rebirth Party (LRP) conservative        \" 'Restoring the Foundin… #>  3 Eunoia Party                    progressive         \"We believe that 'eudaim… #>  4 Libertarian Progressives        conservative        \"Balancing Tradition wit… #>  5 The Luminari                    progressive         \" 'We are the torchbeare… #>  6 Libertas Novi                   conservative        \"We believe that the Uni… #>  7 Eudaimonia                      progressive         \"We believe that the gre… #>  8 The Terra Vita Party            conservative        \"Emphasizing the importa… #>  9 The Luminari Party              progressive         \"We believe that technol… #> 10 The New Order Party (NOP)       conservative        \"Protecting Traditional … category_schema <- list(   type = \"object\",   properties = list(     `political leaning` = list(       type = \"string\",       enum = c(\"progressive\", \"conservative\")     ),     `explanation` = list(type = \"string\")   ),   required = c(     \"political leaning\",     \"explanation\"   ) )  categories_df <- purrr::map(   .x = parties_responses_df[[\"political statement\"]],   .f = \\(current_statement) {     ql_prompt(       prompt = current_statement,       system = \"You identify the political leaning of political parties based on their statements.\",       format = category_schema,       temperature = 0,       model = \"mistral\"     ) |>       ql_generate()   } ) |>   purrr::list_rbind()  categories_responses_df <- purrr::map(   .x = categories_df$response,   .f = \\(x) {     yyjsonr::read_json_str(x) |>       tibble::as_tibble()   } ) |>   purrr::list_rbind()    responses_combo_df <- dplyr::bind_cols(   parties_responses_df |>     dplyr::rename(`given political leaning` = `political leaning`) |>     dplyr::select(`political statement`, `given political leaning`),   categories_responses_df |>     dplyr::rename(`identified political leaning` = `political leaning`) )  responses_combo_df |>   dplyr::mutate(explanation = stringr::str_trunc(explanation, width = 256)) |>   knitr::kable()"},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"pass-images-to-the-model","dir":"","previous_headings":"","what":"Pass images to the model","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"can pass images multimodal models e.g. “llama3.2-vision” (considerably smaller) “llava-phi3” consider response. Just pass path relevant image ql_prompt(). example, ask describe logo package, get following reponse: appears image bird’s head, possibly goose swan, beak open feathers ruffled. image framed pink hexagon shape.","code":"library(\"quackingllama\")  img_path <- fs::path(   system.file(package = \"quackingllama\"),   \"help\",   \"figures\",   \"logo.png\" )  resp_df <- ql_prompt(   prompt = \"what is this?\",   images = img_path,   model = \"llama3.2-vision\" ) |>   ql_generate()   cat(\">\", resp_df$response) resp_df <- ql_prompt(   prompt = \"what is this?\",   images = img_path,   model = \"llava-phi3\" ) |>   ql_generate()   cat(\">\", resp_df$response) #> > The image features a close-up of a llama's face, which is the central focus of the composition. The llama has a distinctive yellow nose and ears that stand out against its white fur. A gray mask with two holes for eyes covers the lower part of the llama's face, adding an element of intrigue to the image. The background is black, providing a stark contrast to the vibrant colors of the llama and enhancing the visibility of the details in the image. The entire scene is framed within a pink border, further emphasizing the subject matter. The image does not contain any text or other discernible objects. The relative position of the objects suggests that the mask is worn by the llama, covering its lower face while leaving its upper facial features exposed."},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"about-context-windows-and-time-outs","dir":"","previous_headings":"","what":"About context windows and time-outs","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"ollama great enabling easy local deployment local LLMs, comes embedded defaults may come unintended consequences.","code":""},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"about-the-context-window","dir":"","previous_headings":"About context windows and time-outs","what":"About the context window","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"look model page one models available Ollama’s website, may well notice come large context windows. example, gemma3 boasts “128K context window”, big enough include book-length inputs. may well expect , default, context window fully available . , however, mistaken: matter model’s capabilities, Ollama truncates input 2048 tokens: user, notice looked ollama serve logs, notice unsatisfying results, truncation happens way mostly invisible client. known issue Ollama, approached sensibly Ollama, user take core limitation (quackingllama likely include dedicated warning future versions). easiest workaround re-create new model larger context window: ’s matter seconds, following instructions reported relevant issue Ollama. Basically, command line something like : matter seconds get gemma3 model 64k context window, ’ll able use choosing gemma3-64k model.","code":"$ ollama run gemma3 >>> /set parameter num_ctx 65536 Set parameter 'num_ctx' to '65536' >>> /save gemma3-64k Created new model 'gemma3-64k' >>> /bye"},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"about-timeout-and-keep_alive","dir":"","previous_headings":"About context windows and time-outs","what":"About timeout and keep_alive","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"Congratulations, now can enjoy bigger context windows. nice, makes also likely going stumble time-issues, processing lengthy prompts can take many minutes. two parameters determine long quackingllama wait response ollama throwing error. one ollama’s keep_alive argument, basically tells long model remain memory called. default, “5m” five minutes. model doesn’t get response time, throws error. one httr2’s timeout argument, expresses long client waiting response. defaults “300”, expressed seconds, corresponds 5 minutes. combined effect two arguments may exactly expect (5 minute keep_alive may actually let model run 10 minutes, timeout argument big enough), either way, mindful expect lengthy response times, set values adequately high value. hand, know short prompts expect quick responses, defaults efficient, just move sooner model stuck whatever reason.","code":""},{"path":"https://giocomai.github.io/quackingllama/index.html","id":"about-the-hex-logo","dir":"","previous_headings":"","what":"About the hex logo","title":"Process text with Ollama, retrieve structured results, cache them locally in DuckDB","text":"logo may may recognise quacking llama, maybe, just llama wearing duck mask. reference obviously two main tools used package: ollama DuckDB. Image generated machine stablediffusion.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_disable_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Disable caching for the current session — ql_disable_db","title":"Disable caching for the current session — ql_disable_db","text":"Disable caching current session","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_disable_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disable caching for the current session — ql_disable_db","text":"","code":"ql_disable_db()"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_disable_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Disable caching for the current session — ql_disable_db","text":"Nothing, used side effects.","code":""},{"path":[]},{"path":"https://giocomai.github.io/quackingllama/reference/ql_disable_db.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Disable caching for the current session — ql_disable_db","text":"","code":"ql_disable_db()"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_enable_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable storing data in a database for the current session — ql_enable_db","title":"Enable storing data in a database for the current session — ql_enable_db","text":"Enable storing data database current session","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_enable_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable storing data in a database for the current session — ql_enable_db","text":"","code":"ql_enable_db(db_type = \"DuckDB\")"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_enable_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable storing data in a database for the current session — ql_enable_db","text":"db_type Defaults DuckDB.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_enable_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enable storing data in a database for the current session — ql_enable_db","text":"Nothing, used side effects.","code":""},{"path":[]},{"path":"https://giocomai.github.io/quackingllama/reference/ql_enable_db.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enable storing data in a database for the current session — ql_enable_db","text":"","code":"ql_enable_db()"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a response and return the result in a data frame — ql_generate","title":"Generate a response and return the result in a data frame — ql_generate","text":"Generate response return result data frame","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a response and return the result in a data frame — ql_generate","text":"","code":"ql_generate(   prompt_df,   only_cached = FALSE,   host = NULL,   message = NULL,   keep_alive = NULL,   timeout = NULL,   error = c(\"fail\", \"warn\") )"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a response and return the result in a data frame — ql_generate","text":"prompt_df data frame inputs passed LLM, typically created ql_prompt(). only_cached Defaults FALSE. TRUE, cached responses returned. host address Ollama API can reached, e.g. http://localhost:11434 locally deployed Ollama. keep_alive Defaults \"5m\". Controls long model stay loaded memory following request. timeout set ql_set_options(), defaults 300 seconds (5 minutes). error Defines errors handled, defaults \"fail\", .e. error emerges querying LLM, function stops. set \"warn\", sets response NA_character_ stores database. can useful e.g. proceed prompts include request routinely time outs without giving response. imply model never give respones, e.g. re-running query longer time may work.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a response and return the result in a data frame — ql_generate","text":"data frame, including response column, well information returned model.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_generate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a response and return the result in a data frame — ql_generate","text":"","code":"if (FALSE) { # \\dontrun{ ql_prompt(\"a haiku\") |>   ql_generate()   } # }"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_db_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve — ql_get_db_options","title":"Retrieve — ql_get_db_options","text":"Retrieve","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_db_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve — ql_get_db_options","text":"","code":"ql_get_db_options(options = c(\"db\", \"db_type\", \"db_folder\", \"db_filename\"))"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_db_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve — ql_get_db_options","text":"options Available options ","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_db_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve — ql_get_db_options","text":"list selected options.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_db_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve — ql_get_db_options","text":"","code":"ql_get_db_options() #> $db #> [1] TRUE #>  #> $db_type #> [1] \"DuckDB\" #>  #> $db_folder #> . #>  #> $db_filename #> [1] \"\" #>   ## Retrieve only selected option ql_get_db_options(\"db_type\") #> $db_type #> [1] \"DuckDB\" #>"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Get available models — ql_get_models","title":"Get available models — ql_get_models","text":"Get available models","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get available models — ql_get_models","text":"","code":"ql_get_models(host = \"http://localhost:11434\")"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get available models — ql_get_models","text":"host Defaults \"http://localhost:11434\", locally deployed Ollama usually responds.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get available models — ql_get_models","text":"data frame (tibble) details locally available models.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get available models — ql_get_models","text":"","code":"if (FALSE) { # \\dontrun{ ql_get_models() } # }"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Get options — ql_get_options","title":"Get options — ql_get_options","text":"Get options","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get options — ql_get_options","text":"","code":"ql_get_options(   options = c(\"system\", \"model\", \"host\", \"temperature\", \"seed\", \"keep_alive\", \"timeout\"),   system = NULL,   model = NULL,   host = NULL,   temperature = NULL,   seed = NULL,   keep_alive = NULL,   timeout = NULL )"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get options — ql_get_options","text":"options character vector used filter options effectively returned. Defaults available.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get options — ql_get_options","text":"list available options (selected options)","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_get_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get options — ql_get_options","text":"","code":"ql_set_options(   model = \"llama3.2\",   host = \"http://localhost:11434\",   system = \"You are a helpful assistant.\",   temperature = 0,   seed = 42,   keep_alive = \"5m\" )  ql_get_options() #> $system #> [1] \"You are a helpful assistant.\" #>  #> $model #> [1] \"llama3.2\" #>  #> $host #> [1] \"http://localhost:11434\" #>  #> $temperature #> [1] 0 #>  #> $seed #> [1] 42 #>  #> $keep_alive #> [1] \"5m\" #>  #> $timeout #> [1] 300 #>"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_hash.html","id":null,"dir":"Reference","previous_headings":"","what":"Hash all inputs relevant to the call to the LLM and create a hash to be used for caching. — ql_hash","title":"Hash all inputs relevant to the call to the LLM and create a hash to be used for caching. — ql_hash","text":"Mostly used internally.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_hash.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hash all inputs relevant to the call to the LLM and create a hash to be used for caching. — ql_hash","text":"","code":"ql_hash(prompt_df)"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_hash.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hash all inputs relevant to the call to the LLM and create a hash to be used for caching. — ql_hash","text":"prompt_df data frame inputs passed LLM, typically created ql_prompt().","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_hash.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hash all inputs relevant to the call to the LLM and create a hash to be used for caching. — ql_hash","text":"tibble, returned ql_prompt(), always including hash column.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_hash.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hash all inputs relevant to the call to the LLM and create a hash to be used for caching. — ql_hash","text":"","code":"ql_prompt(\"a haiku\", hash = FALSE) |> ql_hash() #> # A tibble: 1 × 7 #>   prompt  system                        seed temperature model    format hash    #>   <chr>   <chr>                        <dbl>       <dbl> <chr>    <chr>  <chr>   #> 1 a haiku You are a helpful assistant.     0           0 llama3.2 \"\"     e64e99…"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","title":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","text":"Typically passed ql_generate().","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","text":"","code":"ql_prompt(   prompt,   system = NULL,   format = NULL,   model = NULL,   images = NULL,   temperature = NULL,   seed = NULL,   host = NULL,   hash = TRUE )"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","text":"prompt prompt LLM. system System message pass model. See official documentation details. example: \"helpful assistant.\" model name model, e.g. llama3.2 phi3.5:3.8b. Run ollama list command line see list locally available models. temperature Numeric value comprised 0 1 passed model. set 0 seed, response prompt always exactly . closer one, response variable creative. Use 0 consistent responses. Setting 0.7 common choice creative interactive tasks. seed integer. temperature set 0 seed constant, model consistently returns response prompt. host address Ollama API can reached, e.g. http://localhost:11434 locally deployed Ollama. hash Defaults TRUE. TRUE, adds column hash components prompt. Used internally caching. Can added separately ql_hash().","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","text":"tibble main components query, passed ql_generate().","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_prompt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","text":"details context parameter, see https://github.com/ollama/ollama/blob/main/docs/api.md.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_prompt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a data frame with all relevant inputs for the LLM. — ql_prompt","text":"","code":"ql_prompt(\"a haiku\") #> # A tibble: 1 × 7 #>   prompt  system                        seed temperature model    format hash    #>   <chr>   <chr>                        <dbl>       <dbl> <chr>    <chr>  <chr>   #> 1 a haiku You are a helpful assistant.     0           0 llama3.2 \"\"     e64e99…"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_read_images.html","id":null,"dir":"Reference","previous_headings":"","what":"Read image in order to pass it to multimodal models — ql_read_images","title":"Read image in order to pass it to multimodal models — ql_read_images","text":"Read image order pass multimodal models","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_read_images.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read image in order to pass it to multimodal models — ql_read_images","text":"","code":"ql_read_images(path)"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_read_images.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read image in order to pass it to multimodal models — ql_read_images","text":"path Path image file.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_read_images.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read image in order to pass it to multimodal models — ql_read_images","text":"list object character vectors base 64 encoded images.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_read_images.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read image in order to pass it to multimodal models — ql_read_images","text":"","code":"if (interactive()) {   library(\"quackingllama\")    img_path <- fs::file_temp(ext = \"png\")    download.file(     url = \"https://ollama.com/public/ollama.png\",     destfile = img_path   )    resp_df <- ql_prompt(     prompt = \"what is this?\",     images = img_path,     model = \"llama3.2-vision\"   ) |>     ql_generate()    resp_df    resp_df$response }"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Create httr2 request for both generate and chat endpoints — ql_request","title":"Create httr2 request for both generate and chat endpoints — ql_request","text":"Create httr2 request generate chat endpoints","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create httr2 request for both generate and chat endpoints — ql_request","text":"","code":"ql_request(   prompt_df,   endpoint = \"generate\",   host = NULL,   message = NULL,   keep_alive = NULL,   timeout = NULL )"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create httr2 request for both generate and chat endpoints — ql_request","text":"endpoint Defaults \"generate\". Must either \"generate\" \"chat\". host address Ollama API can reached, e.g. http://localhost:11434 locally deployed Ollama. keep_alive Defaults \"5m\". Controls long model stay loaded memory following request. timeout set ql_set_options(), defaults 300 seconds (5 minutes).","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create httr2 request for both generate and chat endpoints — ql_request","text":"httr2 request object.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create httr2 request for both generate and chat endpoints — ql_request","text":"","code":"ql_prompt(prompt = \"a haiku\") #> # A tibble: 1 × 7 #>   prompt  system                        seed temperature model    format hash    #>   <chr>   <chr>                        <dbl>       <dbl> <chr>    <chr>  <chr>   #> 1 a haiku You are a helpful assistant.     0           0 llama3.2 \"\"     e64e99…  ql_prompt(prompt = \"a haiku\") |>   ql_request() |>   httr2::req_dry_run() #> POST /api/generate HTTP/1.1 #> accept: */* #> accept-encoding: deflate, gzip, br, zstd #> content-length: 175 #> content-type: application/json #> host: localhost:11434 #> user-agent: httr2/1.1.2 r-curl/6.2.2 libcurl/8.5.0 #>  #> { #>   \"model\": \"llama3.2\", #>   \"prompt\": \"a haiku\", #>   \"images\": null, #>   \"stream\": false, #>   \"raw\": false, #>   \"keep_alive\": \"5m\", #>   \"options\": { #>     \"seed\": 0, #>     \"temperature\": 0 #>   }, #>   \"system\": \"You are a helpful assistant.\" #> }"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_db_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Set options for the local database and enables caching — ql_set_db_options","title":"Set options for the local database and enables caching — ql_set_db_options","text":"Set options local database enables caching","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_db_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set options for the local database and enables caching — ql_set_db_options","text":"","code":"ql_set_db_options(db_filename = NULL, db_type = \"DuckDB\", db_folder = \".\")"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_db_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set options for the local database and enables caching — ql_set_db_options","text":"db_filename Defaults NULL. Internally, defaults combination quackingllama, followed name model used. Name given local database file. Useful differentiating among different approaches projects storing multiple database files folder. db_type Defaults DuckDB. db_folder Defaults ., .e., current working directory.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_db_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set options for the local database and enables caching — ql_set_db_options","text":"Nothing, used side effects.","code":""},{"path":[]},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_db_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set options for the local database and enables caching — ql_set_db_options","text":"","code":"ql_set_db_options(db_filename = \"testing_ground\")"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Set basic options for the current session. — ql_set_options","title":"Set basic options for the current session. — ql_set_options","text":"Set basic options current session.","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set basic options for the current session. — ql_set_options","text":"","code":"ql_set_options(   system = NULL,   model = NULL,   host = NULL,   temperature = NULL,   seed = NULL,   keep_alive = NULL,   timeout = NULL )"},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set basic options for the current session. — ql_set_options","text":"system System message pass model. See official documentation details. example: \"helpful assistant.\" model name model, e.g. llama3.2 phi3.5:3.8b. Run ollama list command line see list locally available models. host address Ollama API can reached, e.g. http://localhost:11434 locally deployed Ollama. temperature Numeric value comprised 0 1 passed model. set 0 seed, response prompt always exactly . closer one, response variable creative. Use 0 consistent responses. Setting 0.7 common choice creative interactive tasks. seed integer. temperature set 0 seed constant, model consistently returns response prompt. keep_alive Defaults \"5m\". Controls long model stay loaded memory following request. timeout Time seconds request times . Defaults 300 (corresponding 5 minutes).","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set basic options for the current session. — ql_set_options","text":"Nothing, used side effects. Options can retrieved ql_get_db_options()","code":""},{"path":"https://giocomai.github.io/quackingllama/reference/ql_set_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set basic options for the current session. — ql_set_options","text":"","code":"ql_set_options(   model = \"llama3.2\",   host = \"http://localhost:11434\",   system = \"You are a helpful assistant.\",   temperature = 0,   seed = 42 )  ql_get_options() #> $system #> [1] \"You are a helpful assistant.\" #>  #> $model #> [1] \"llama3.2\" #>  #> $host #> [1] \"http://localhost:11434\" #>  #> $temperature #> [1] 0 #>  #> $seed #> [1] 42 #>  #> $keep_alive #> [1] \"5m\" #>  #> $timeout #> [1] 300 #>"},{"path":"https://giocomai.github.io/quackingllama/reference/quackingllama-package.html","id":null,"dir":"Reference","previous_headings":"","what":"quackingllama: Process text with Ollama, retrieve structured results, cache them locally in DuckDB — quackingllama-package","title":"quackingllama: Process text with Ollama, retrieve structured results, cache them locally in DuckDB — quackingllama-package","text":"Process text Ollama, store results DuckDB.","code":""},{"path":[]},{"path":"https://giocomai.github.io/quackingllama/reference/quackingllama-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"quackingllama: Process text with Ollama, retrieve structured results, cache them locally in DuckDB — quackingllama-package","text":"Maintainer: Giorgio Comai g@giorgiocomai.eu (ORCID) [copyright holder]","code":""}]
