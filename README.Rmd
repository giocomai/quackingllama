---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# quackingllama <img src="man/figures/logo.png" align="right" height="240" alt="quackingllama logo - A llama with a duck mask in a hexagon" />

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

The goal of `quackingllama` is to facilitate efficient interactions with LLMs; its current target use-case is text classification (e.g. categorise or tag contents, or extract information from text). Key features include:

- facilitate consistently formatted responses (through [Ollama's structured ouputs](https://ollama.com/blog/structured-outputs))
- facilitate local caching (by storing results in a local `DuckDB` database)
- facilitate initiating text classification tasks (through examples and convenience functions)
- facilitate keeping a record with details about how each response has been received


## Installation

You can install the development version of `quackingllama` from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("giocomai/quackingllama")
```

## Default options and outputs

In order to facilitate consistent results, by default `quackingllama` sets the temperature of the model to 0: this means that it will always return the same response when given the same prompt. When caching is enabled, responses can then consistently be retrieved from the local cache without querying again the LLMs. 

All functions consistently return results in a data frame (a tibble).

Key functionalities will be demonstrated through a series of examples.

As the package is developed further, some of the less intuitive tasks (e.g. defining a schema) will be facilitated through dedicated convenience functions.


## Basic examples

### Text generation


```{r example}
library("quackingllama")
```

Let's generate a short piece of text. Results are returned in a data frame, with the `response` in the first column and all relevant metadata about the query stored along with it. 

```{r pol_df}
pol_df <- ql_prompt(prompt = "Describe an imaginary political leader in less than 100 words.") |>
  ql_generate()

str(pol_df)
```

```{r pol_df response, results='asis'}
cat(">", pol_df$response)
```

If we are interested in variations of this text, we can easily create them:

```{r pol3_df}
# TODO accept multiple prompts by default

pol3_df <- purrr::map(
  .x = c("progressive", "conservative", "centrist"),
  .f = \(x) {
    ql_prompt(prompt = glue::glue("Describe an imaginary {x} politician in less than 100 words.")) |>
      ql_generate()
  }
) |>
  purrr::list_rbind()

pol3_df$response
```

These are, as it is the customary default behaviour of LLMs, free form texts. Depending on the task at hand, we may want to have text in a more structured format. To do so, we must provide the LLM with a [schema](https://json-schema.org/) of how we want it to to return data. 

Schema can be very simple, e.g., if we want our response to feature only a "name" and "description" field, and both should be character strings, we'd use the following schema:


```{r basic_schema}
# TODO convenience function to facilitate creation of common schemas

schema <- list(
  type = "object",
  properties = list(
    `name` = list(type = "string"),
    `description` = list(type = "string")
  ),
  required = c("name", "description")
)
```


```{r pol_schema_df basic}
pol_schema_df <- ql_prompt(
  prompt = "Describe an imaginary political leader.",
  format = schema
) |>
  ql_generate()

pol_schema_df$response |>
  yyjsonr::read_json_str()
```


or slightly more complex, for example making clear that we expect a field to be numeric, and another one to pick between one of a set of options:



```{r medium_schema}
schema <- list(
  type = "object",
  properties = list(
    `name` = list(type = "string"),
    `age` = list(type = "number"),
    `gender` = list(
      type = "string",
      enum = c("female", "male", "non-binary")
    ),
    `motto` = list(type = "string"),
    `description` = list(type = "string")
  ),
  required = c(
    "name",
    "age",
    "gender",
    "motto",
    "description"
  )
)
```

And the returned is formatted as expected:

```{r pol_schema_df medium}
pol_schema_df <- ql_prompt(
  prompt = "Describe an imaginary political leader.",
  format = schema
) |>
  ql_generate()

pol_schema_df$response |>
  yyjsonr::read_json_str()
```


Having the response in a structured format allows for easily storing results in a data frame and processing them further. 


```{r pol3_schema_df}
pol3_schema_df <- purrr::map(
  .x = c("progressive", "conservative", "centrist"),
  .f = \(x) {
    ql_prompt(
      prompt = glue::glue("Describe an imaginary {x} politician."),
      format = schema
    ) |>
      ql_generate()
  }
) |>
  purrr::list_rbind()

pol3_schema_responses_df <- purrr::map(
  .x = pol3_schema_df$response,
  .f = \(x) {
    yyjsonr::read_json_str(x) |>
      tibble::as_tibble()
  }
) |>
  purrr::list_rbind()

pol3_schema_responses_df
```

This has obvious advantages for many data processing tasks, and, as will be seen, can effectively be used to enhance the consistency of text classification tasks. But first, let's discuss caching and some of the options that determine output. 

### Caching and options

So far, local caching has not been enabled: this means that even when the exacts same response is expected, this will still be requested to the LLM, which can be exceedingly time-consuming especially for repetitive tasks, or for data processing pipelines that may recurrently encounter the same data.

Caching is the obvious answer to this process, but when do we expect exactly the same response from the LLM, considering that LLMs do not necessarily return the same response even when given the same prompt?

Two parameters are particularly relevant for understanding this, `temperature` and `seed`.

What is "temperature"? [Ollama's documentation](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) concisely clarifies the effect of this parameter by suggesting that "Increasing the temperature will make the model answer more creatively." LLMs often have the default temperature set to 0.7 or 0.8. In brief, when temperature is set to its maximum value of 1, the LLMs will provide more varied responses. When temperature is set to 0, the LLMs are at their more consistent: they always provide the same response to the same prompt.

What does it mean in practices? For example, that if I set the temperature to 0 and ask the same LLM to generate a haiku, I will always get the very same haiku, no matter how many times I run this command.

```{r funny_haiku_temp0}
ql_prompt(prompt = "A reasonably funny haiku", temperature = 0) |>
  ql_generate() |>
  dplyr::pull(response)
ql_prompt(prompt = "A reasonably funny haiku", temperature = 0) |>
  ql_generate() |>
  dplyr::pull(response)
ql_prompt(prompt = "A reasonably funny haiku", temperature = 0) |>
  ql_generate() |>
  dplyr::pull(response)
```

If I set the temperature to 1, I get every time a different haiku (ok, not very different, really, but still different).

```{r funny_haiku_temp1}
ql_prompt(prompt = "A reasonably funny haiku", temperature = 1) |>
  ql_generate() |>
  dplyr::pull(response)
ql_prompt(prompt = "A reasonably funny haiku", temperature = 1) |>
  ql_generate() |>
  dplyr::pull(response)
ql_prompt(prompt = "A reasonably funny haiku", temperature = 1) |>
  ql_generate() |>
  dplyr::pull(response)
```

But then, replicability of results is possible even when the temperature is set to a value higher than 0. We just need to set the same seed, and we'll consistently get the same result.

```{r funny_haiku_temp1_seed2024}
ql_prompt(prompt = "A reasonably funny haiku", temperature = 1, seed = 2025) |>
  ql_generate() |>
  dplyr::pull(response)
ql_prompt(prompt = "A reasonably funny haiku", temperature = 1, seed = 2025) |>
  ql_generate() |>
  dplyr::pull(response)
ql_prompt(prompt = "A reasonably funny haiku", temperature = 1, seed = 2025) |>
  ql_generate() |>
  dplyr::pull(response)
```

Two additional components determine if the response is exactly the same in different instances: `system` and `format`. The `system` parameter is passed along with the prompt to the LLM, and by default is set to the generic "You are a helpful assistant.". This is a reasonable generic option, but there may be good reasons to be more specific depending on the task at hand. 

For example, if we set as the system message "You are a 19th century romantic poet.", the style of the response will change (somewhat) accordingly. 

```{r funny_haiku_temp0_19th_century}
ql_prompt(
  prompt = "A reasonably funny haiku",
  temperature = 0,
  system = "You are a 19th century romantic writer."
) |>
  ql_generate() |>
  dplyr::pull(response)
```

As discussed above, `format` is relevant only for instances when a structured output is requested to the LLM by providing a schema. For example, if we provided a different schema, the output would also have been different.

```{r different schema}
schema <- list(
  type = "object",
  properties = list(
    `haiku` = list(type = "string"),
    `why_funny` = list(type = "string")
  ),
  required = c(
    "haiku",
    "why_funny"
  )
)

haiku_str_df <- ql_prompt(
  prompt = "Write a funny haiku, and explain why it is supposed to be funny.",
  format = schema
) |> ql_generate()

haiku_str_df |>
  dplyr::pull(response) |>
  yyjsonr::read_json_str()
```


In brief, when should we expect to receive exactly the same response from the LLM, hence, making it possible to retrieve it from cache if already parsed? The following conditions must apply:

- same model
- same `system` parameter
- same `format`, i.e., same schema (if given).
- same prompt
- and
  - either the same seed and any value for `temperature` OR
  - any seed and `temperature` set to zero

If the above conditions are met, and caching is enabled, the response will be retrieved from the local cache, rather than from the LLM. 

It's easy to enable caching for the current session with `ql_enable_db()`. By default, the database is stored in the current working directory, but this can be changed with `ql_set_db_options()`.


```{r enable db}
ql_enable_db()
ql_set_db_options(db_folder = fs::path_home_r("R"))
```

Now even prompts that would take the LLM many seconds to process can be returned efficiently from cache:


### Text classification

First, let's create some texts that we will then try to classify:

```{r parties_responses_df}
schema <- list(
  type = "object",
  properties = list(
    `party name` = list(type = "string"),
    `political leaning` = list(
      type = "string",
      enum = c("progressive", "conservative")
    ),
    `political statement` = list(type = "string")
  ),
  required = c(
    "party name",
    "political leaning",
    "political statement"
  )
)

parties_df <- purrr::map2(
  .x = rep(c("progressive", "conservative"), 5),
  .y = 1:10,
  .f = \(x, y) {
    ql_prompt(
      prompt = glue::glue("Describe an imaginary {x} political party, inventing their party name and a characteristic political statement."),
      format = schema,
      temperature = 1,
      seed = y
    ) |>
      ql_generate()
  }
) |>
  purrr::list_rbind()

parties_responses_df <- purrr::map(
  .x = parties_df$response,
  .f = \(x) {
    yyjsonr::read_json_str(x) |>
      tibble::as_tibble()
  }
) |>
  purrr::list_rbind()

parties_responses_df
```

Then we ask a different model to categorise results (in this example, text generation with `llama3.2`, text categorisation with `mistral`). Trimming explanations in the following table for clarity.

```{r categories_df mistral}
category_schema <- list(
  type = "object",
  properties = list(
    `political leaning` = list(
      type = "string",
      enum = c("progressive", "conservative")
    ),
    `explanation` = list(type = "string")
  ),
  required = c(
    "political leaning",
    "explanation"
  )
)

categories_df <- purrr::map(
  .x = parties_responses_df[["political statement"]],
  .f = \(current_statement) {
    ql_prompt(
      prompt = current_statement,
      system = "You identify the political leaning of political parties based on their statements.",
      format = category_schema,
      temperature = 0,
      model = "mistral"
    ) |>
      ql_generate()
  }
) |>
  purrr::list_rbind()

categories_responses_df <- purrr::map(
  .x = categories_df$response,
  .f = \(x) {
    yyjsonr::read_json_str(x) |>
      tibble::as_tibble()
  }
) |>
  purrr::list_rbind()



responses_combo_df <- dplyr::bind_cols(
  parties_responses_df |>
    dplyr::rename(`given political leaning` = `political leaning`) |>
    dplyr::select(`political statement`, `given political leaning`),
  categories_responses_df |>
    dplyr::rename(`identified political leaning` = `political leaning`)
)

responses_combo_df |>
  dplyr::mutate(explanation = stringr::str_trunc(explanation, width = 256)) |>
  knitr::kable()
```


In this stereotyped case, the LLM categorises most statements as expected and provide a broadly meaningful explanation for the choice (if you try with shorter sentences, e.g., just a political motto, the correct response rate decreases substantially). Fundamentally:

- responses are returned in a predictable and user-defined format, consistently responding with user-defined categories
- responses are cached locally:
  - re-running a categorisation task is efficient
  - the categorisation of a large set of texts can be interrupted at will, and already processed contents will not be categorised again.


Querying with different models can have a substantial impact on the quality of results. 

## Pass images to the model

You can pass images and have multimodal models such as e.g. "llama3.2-vision" or (the considerably smaller) "llava-phi3" consider them in their response. Just pass the path of the relevant image to `ql_prompt()`. For example, if we ask to describe the logo of this package, we get the following reponse:

```{r describe images, results='asis'}
library("quackingllama")

img_path <- fs::path(
  system.file(package = "quackingllama"),
  "help",
  "figures",
  "logo.png"
)

resp_df <- ql_prompt(
  prompt = "what is this?",
  images = img_path,
  model = "llama3.2-vision"
) |>
  ql_generate()


cat(">", resp_df$response)
```

```{r llava phi3}
resp_df <- ql_prompt(
  prompt = "what is this?",
  images = img_path,
  model = "llava-phi3"
) |>
  ql_generate()


cat(">", resp_df$response)
```

## Thinking models

In May 2025, Ollama started supporting "thinking" models ([more details in the post announcing the feature](https://ollama.com/blog/thinking)). Pay attention to the fact that not all reasoning models available via Ollama actually support thinking mode; as of July 2025, only three models were effectively supported (`deepseek-r1`, `qwen3`, and `magistral`). An [up-to-date list should be available on Ollama's website](https://ollama.com/search?c=thinking).

When thinking mode is enabled, the LLM goes through an iterative "thinking" process before providing its answer. The "thinking" process is expressed in plain English and can be seen along with the response. See the following example:


```{r strawberry_t_df}
strawberry_t_df <- ql_prompt(
  prompt = "How many r are there in strawberry? Provide a concise answer.",
  model = "deepseek-r1:1.5b",
  think = TRUE) |>
  ql_generate()

```

Here's the thinking:

```{r strawberry_t_df thinking, results='asis'}

cat(">", strawberry_t_df$thinking |> stringr::str_replace_all(pattern = stringr::fixed("\n"), replacement = "\n > ")) 
```

And here is the response:

```{r strawberry_t_df response, results='asis'}
cat(">", strawberry_t_df$response)
```


## About context windows and time-outs

`ollama` is great in enabling easy local deployment of local LLMs, but comes with some embedded defaults that may come with unintended consequences. 

### About the context window

If you look at the model page of one of the models available from Ollama's website, you may well notice that some of these come with very large context windows. For example, [`gemma3`](https://ollama.com/library/gemma3) boasts a "128K context window", big enough to include book-length inputs. You may well expect that, by default, this context window is fully available to you. You would, however, be mistaken: no matter the model's capabilities, Ollama truncates the input at 2048 tokens: as a user, you would notice it only if you looked at the `ollama serve` logs, or because you notice unsatisfying results, as truncation happens in a way that is mostly invisible to the client. This is a known issue with Ollama, and until this is approached more sensibly by Ollama, the user should take core of this limitation themselves (`quackingllama` will likely include a dedicated warning in future versions). The easiest workaround is to re-create a new model with a larger context window: it's a matter of a few seconds, following the [instructions reported in the relevant issue on Ollama](https://github.com/ollama/ollama/issues/8099#issuecomment-2543316682). 

Basically, from the command line you do something like this:
```
$ ollama run gemma3
>>> /set parameter num_ctx 65536
Set parameter 'num_ctx' to '65536'
>>> /save gemma3-64k
Created new model 'gemma3-64k'
>>> /bye
```
And in a matter of seconds you will get a `gemma3` model with a 64k context window, which you'll be able to use by choosing `gemma3-64k` as model.

### About `timeout` and `keep_alive`

Congratulations, now you can enjoy bigger context windows. This is all nice, but this makes it also more likely that you are going to stumble into time-out issues, as processing lengthy prompts can take many minutes. 

There are two parameters that determine how long `quackingllama` will wait for a response from `ollama` before throwing an error.

- one is `ollama`'s `keep_alive` argument, that basically tells how long the model should remain in memory after it is called. By default, this is "5m" for five minutes. If the model doesn't get a response in time, it throws an error. 
- one is `httr2`'s `timeout` argument, that expresses how long the client should be waiting for a response. This defaults to "300", as it is expressed in seconds, and corresponds to 5 minutes. 

The combined effect of these two arguments may not be exactly as you expect (a 5 minute `keep_alive` may actually let the model run for 10 minutes, if your `timeout` argument is big enough), but either way, be mindful and if you do expect lengthy response times, do set both values to an adequately high value. 

On the other hand, if you know you have short prompts and expect quick responses, the defaults are more efficient, and will just move on sooner if the model is stuck for whatever reason. 


## About the hex logo

In the logo you may or may not recognise a quacking llama, or maybe, just a llama wearing a duck mask. The reference is obviously to two of the main tools used by this package: [`ollama`](https://ollama.com/) and [`DuckDB`](https://duckdb.org/docs/api/r.html). Image generated on my machine with `stablediffusion`.
